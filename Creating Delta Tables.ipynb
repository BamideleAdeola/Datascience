{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CREATING A DELTA TABLE USING MICROSOFT FABRIC**\n",
    "\n",
    "- This below shows a simple way of creating a table and saving it as a Delta format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# **Create Delta Tables**\n",
    "With apache spark, you can create a delta table which will be stored in underlining Parquet files for the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**1. Create Delta Table from Dataframe**\n",
    "\n",
    "One of the easiest way of creating a delta table in Spark is to save a dataframe in the **delta format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-07-05T01:16:03.6563998Z",
       "execution_start_time": "2023-07-05T01:15:41.3050316Z",
       "livy_statement_state": "available",
       "parent_msg_id": "6b15e1d5-a6a8-4dfd-a15a-354bf4e2bbda",
       "queued_time": "2023-07-05T01:15:40.8959527Z",
       "session_id": "f2a86753-f390-439a-b815-2d9b325186ca",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-07-05T01:16:00.190GMT",
          "dataRead": 4442,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 4:\n# First let's load a file into a dataframe\ndf = spark.read.load('Files/measles.csv', format = 'csv', header = True)\n\n# Then save the dataframe as a delta format\ndf.write.format(\"delta\").saveAsTable(\"MyDeltaTable\"): Compute snapshot for version: 0",
          "jobGroup": "4",
          "jobId": 11,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 51,
          "numTasks": 52,
          "rowCount": 50,
          "stageIds": [
           15,
           16,
           17
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-07-05T01:16:00.150GMT"
         },
         {
          "completionTime": "2023-07-05T01:16:00.124GMT",
          "dataRead": 3011,
          "dataWritten": 4442,
          "description": "Delta: Job group for statement 4:\n# First let's load a file into a dataframe\ndf = spark.read.load('Files/measles.csv', format = 'csv', header = True)\n\n# Then save the dataframe as a delta format\ndf.write.format(\"delta\").saveAsTable(\"MyDeltaTable\"): Compute snapshot for version: 0",
          "jobGroup": "4",
          "jobId": 10,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 55,
          "stageIds": [
           13,
           14
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-07-05T01:15:59.276GMT"
         },
         {
          "completionTime": "2023-07-05T01:15:58.994GMT",
          "dataRead": 4244,
          "dataWritten": 3011,
          "description": "Delta: Job group for statement 4:\n# First let's load a file into a dataframe\ndf = spark.read.load('Files/measles.csv', format = 'csv', header = True)\n\n# Then save the dataframe as a delta format\ndf.write.format(\"delta\").saveAsTable(\"MyDeltaTable\"): Compute snapshot for version: 0",
          "jobGroup": "4",
          "jobId": 9,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 10,
          "stageIds": [
           12
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-07-05T01:15:58.896GMT"
         },
         {
          "completionTime": "2023-07-05T01:15:58.356GMT",
          "dataRead": 7377963,
          "dataWritten": 2085648,
          "description": "Job group for statement 4:\n# First let's load a file into a dataframe\ndf = spark.read.load('Files/measles.csv', format = 'csv', header = True)\n\n# Then save the dataframe as a delta format\ndf.write.format(\"delta\").saveAsTable(\"MyDeltaTable\")",
          "jobGroup": "4",
          "jobId": 8,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 2,
          "numCompletedStages": 1,
          "numCompletedTasks": 2,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 2,
          "rowCount": 132226,
          "stageIds": [
           11
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-07-05T01:15:57.105GMT"
         },
         {
          "completionTime": "2023-07-05T01:15:43.414GMT",
          "dataRead": 65536,
          "dataWritten": 0,
          "description": "Job group for statement 4:\n# First let's load a file into a dataframe\ndf = spark.read.load('Files/measles.csv', format = 'csv', header = True)\n\n# Then save the dataframe as a delta format\ndf.write.format(\"delta\").saveAsTable(\"MyDeltaTable\")",
          "jobGroup": "4",
          "jobId": 7,
          "killedTasksSummary": {},
          "name": "load at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 1,
          "stageIds": [
           10
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-07-05T01:15:42.509GMT"
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 5,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 4
      },
      "text/plain": [
       "StatementMeta(, f2a86753-f390-439a-b815-2d9b325186ca, 4, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First let's load a file into a dataframe\n",
    "df = spark.read.load('Files/measles.csv', format = 'csv', header = True)\n",
    "\n",
    "# Then save the dataframe as a delta format\n",
    "df.write.format(\"delta\").saveAsTable(\"MyDeltaTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "You can also create a table as an external tables, where the relational table definition \n",
    "in the metastore is mapped to an alternative file storage location\n",
    "\"\"\"\n",
    "\n",
    "df.write.format('delta').saveAsTable(\"myexternalsample\", path = \"Files/externaltable\")\n",
    "\n",
    "# The above can also be modified as a fully qualified path for astorage location\n",
    "df.write.format(\"delta\").saveAsTable(\"myexternalsample\", path =  \"abffff://my_store_url..../externaltable\")\n",
    "#here deleting an external table from the lakehouse **does not** delete the associated data files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# **Creating table metadata**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**1. DeltaTableBuilder API Approach** \n",
    "\n",
    "This will enables spark to create a table based on one's specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {},
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-07-05T01:36:17.3896126Z",
       "execution_start_time": "2023-07-05T01:36:17.0135499Z",
       "livy_statement_state": "available",
       "parent_msg_id": "29d6f53b-5782-4be5-9471-80e34d02c548",
       "queued_time": "2023-07-05T01:36:16.487015Z",
       "session_id": "f2a86753-f390-439a-b815-2d9b325186ca",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 7
      },
      "text/plain": [
       "StatementMeta(, f2a86753-f390-439a-b815-2d9b325186ca, 7, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "DeltaTable.create(spark) \\\n",
    "    .tableName(\"products\") \\\n",
    "    .addColumn(\"Productsid\", \"INT\") \\\n",
    "    .addColumn(\"ProductName\", \"STRING\") \\\n",
    "    .addColumn(\"Category\", \"STRING\") \\\n",
    "    .addColumn(\"Price\", \"FLOAT\") \\\n",
    "    .execute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**2. Spark SQL Approach** \n",
    "\n",
    "This will enables spark **SQL CREATE TABLE STATEMENT** to create a table based on one's specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "sparksql"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE testsales\n",
    "(\n",
    "    Orderid INT NOT NULL,\n",
    "    OrderDate TIMESTAMP NOT NULL,\n",
    "    CustomerName STRING,\n",
    "    SalesTotal FLOAT NOT NULL\n",
    ")\n",
    "USING DELTA\n",
    "--  The above is a manage table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "sparksql"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "-- To create an external table, use the below SQL statement\n",
    "\n",
    "CREATE TABLE MyExternalTableTest\n",
    "USING DELTA\n",
    "LOCATION 'Files/mydataFolder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# **Save Data in Delta Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# To save a dataframe to a new folder location in delta format; Use the code below \n",
    "\n",
    "delta_path = \"Files/mydataTable\"\n",
    "df.write.format(\"delta\").save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Assuming you have created the delta_path = \"Files/mydataTable\" previously\n",
    "# You can replace the content of an existing folder in a dataframe using the **Overwrite** mode\n",
    "latest_df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Adding rows from a dataframe to an existing folder is possible by using the append mode\n",
    "new_rows_df.write.format(\"delta\").mode(\"append\").save(delta_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# **Working with Delta Tables in Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**1. Using Spark SQL** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"INSERT INTO products VALUES (1, 'Widget', 'Accessories', 2.99)\")\n",
    "\n",
    "#  Alternatively you can use the %%sql to call Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**2. Using the Delta API** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a DeltaTable object\n",
    "delta_path = \"Files/mytable\"\n",
    "deltaTable = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Update the table (reduce price of accessories by 10%)\n",
    "deltaTable.update(\n",
    "    condition = \"Category == 'Accessories'\",\n",
    "    set = { \"Price\": \"Price * 0.9\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**3.  Use time travel to work with table versioning **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "sparksql"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2023-07-06T02:29:34.6086316Z",
       "execution_start_time": "2023-07-06T02:29:32.0180099Z",
       "livy_statement_state": "available",
       "parent_msg_id": "e6f7e86f-22c6-4ad7-9bdc-623b3b636b9a",
       "queued_time": "2023-07-06T02:29:31.6917175Z",
       "session_id": "830ddfff-3726-4f5e-b6a6-39a5bd6b4fd7",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2023-07-06T02:29:34.249GMT",
          "dataRead": 0,
          "dataWritten": 0,
          "description": "Job group for statement 6:\n\nDESCRIBE HISTORY sales",
          "jobGroup": "6",
          "jobId": 18,
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 8,
          "numCompletedStages": 1,
          "numCompletedTasks": 8,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 8,
          "rowCount": 1,
          "stageIds": [
           30
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-07-06T02:29:34.138GMT"
         },
         {
          "completionTime": "2023-07-06T02:29:33.991GMT",
          "dataRead": 4440,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 6:\n\nDESCRIBE HISTORY sales: Compute snapshot for version: 0",
          "jobGroup": "6",
          "jobId": 17,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 51,
          "numTasks": 52,
          "rowCount": 50,
          "stageIds": [
           27,
           28,
           29
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-07-06T02:29:33.953GMT"
         },
         {
          "completionTime": "2023-07-06T02:29:33.930GMT",
          "dataRead": 1941,
          "dataWritten": 4440,
          "description": "Delta: Job group for statement 6:\n\nDESCRIBE HISTORY sales: Compute snapshot for version: 0",
          "jobGroup": "6",
          "jobId": 16,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 1,
          "numTasks": 51,
          "rowCount": 54,
          "stageIds": [
           25,
           26
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-07-06T02:29:33.392GMT"
         },
         {
          "completionTime": "2023-07-06T02:29:33.239GMT",
          "dataRead": 2437,
          "dataWritten": 1941,
          "description": "Delta: Job group for statement 6:\n\nDESCRIBE HISTORY sales: Compute snapshot for version: 0",
          "jobGroup": "6",
          "jobId": 15,
          "killedTasksSummary": {},
          "name": "toString at String.java:2994",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 1,
          "rowCount": 8,
          "stageIds": [
           24
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2023-07-06T02:29:33.162GMT"
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 4,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6
      },
      "text/plain": [
       "StatementMeta(, 830ddfff-3726-4f5e-b6a6-39a5bd6b4fd7, 6, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.synapse.sparksql-result+json": {
       "data": [
        [
         "0",
         "2023-07-02T09:07:42Z",
         null,
         null,
         "CREATE OR REPLACE TABLE AS SELECT",
         {
          "description": null,
          "isManaged": "true",
          "partitionBy": "[]",
          "properties": "{}"
         },
         null,
         null,
         null,
         null,
         "Serializable",
         false,
         {
          "numFiles": "1",
          "numOutputBytes": "506545",
          "numOutputRows": "32718"
         },
         null,
         "Apache-Spark/3.3.1.5.2-92314920 Delta-Lake/2.2.0.4"
        ]
       ],
       "schema": {
        "fields": [
         {
          "metadata": {},
          "name": "version",
          "nullable": true,
          "type": "long"
         },
         {
          "metadata": {},
          "name": "timestamp",
          "nullable": true,
          "type": "timestamp"
         },
         {
          "metadata": {},
          "name": "userId",
          "nullable": true,
          "type": "string"
         },
         {
          "metadata": {},
          "name": "userName",
          "nullable": true,
          "type": "string"
         },
         {
          "metadata": {},
          "name": "operation",
          "nullable": true,
          "type": "string"
         },
         {
          "metadata": {},
          "name": "operationParameters",
          "nullable": true,
          "type": {
           "keyType": "string",
           "type": "map",
           "valueContainsNull": true,
           "valueType": "string"
          }
         },
         {
          "metadata": {},
          "name": "job",
          "nullable": true,
          "type": {
           "fields": [
            {
             "metadata": {},
             "name": "jobId",
             "nullable": true,
             "type": "string"
            },
            {
             "metadata": {},
             "name": "jobName",
             "nullable": true,
             "type": "string"
            },
            {
             "metadata": {},
             "name": "runId",
             "nullable": true,
             "type": "string"
            },
            {
             "metadata": {},
             "name": "jobOwnerId",
             "nullable": true,
             "type": "string"
            },
            {
             "metadata": {},
             "name": "triggerType",
             "nullable": true,
             "type": "string"
            }
           ],
           "type": "struct"
          }
         },
         {
          "metadata": {},
          "name": "notebook",
          "nullable": true,
          "type": {
           "fields": [
            {
             "metadata": {},
             "name": "notebookId",
             "nullable": true,
             "type": "string"
            }
           ],
           "type": "struct"
          }
         },
         {
          "metadata": {},
          "name": "clusterId",
          "nullable": true,
          "type": "string"
         },
         {
          "metadata": {},
          "name": "readVersion",
          "nullable": true,
          "type": "long"
         },
         {
          "metadata": {},
          "name": "isolationLevel",
          "nullable": true,
          "type": "string"
         },
         {
          "metadata": {},
          "name": "isBlindAppend",
          "nullable": true,
          "type": "boolean"
         },
         {
          "metadata": {},
          "name": "operationMetrics",
          "nullable": true,
          "type": {
           "keyType": "string",
           "type": "map",
           "valueContainsNull": true,
           "valueType": "string"
          }
         },
         {
          "metadata": {},
          "name": "userMetadata",
          "nullable": true,
          "type": "string"
         },
         {
          "metadata": {},
          "name": "engineInfo",
          "nullable": true,
          "type": "string"
         }
        ],
        "type": "struct"
       }
      },
      "text/plain": [
       "<Spark SQL result set with 1 rows and 15 fields>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "DESCRIBE HISTORY sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "sparksql"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DESCRIBE HISTORY 'Files/mytable'\n",
    "--  use the above code for an external tables history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# You can also specify a specific delta file version into a dataframe using VersionAsOf Option\n",
    "\n",
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Alternatively, use timestampAsOf option\n",
    "df = spark.read.format(\"delta\").option(\"timestampAsOf\", '2022-01-01').load(delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### PREPARED BY BAMIDELE AJAMU\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "notebook_environment": {},
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {},
    "enableDebugMode": false
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "trident": {
   "lakehouse": {
    "default_lakehouse": "7a50a8b4-1a06-410a-945e-9d0db53c9328",
    "default_lakehouse_name": "Sales_LakeHouse",
    "default_lakehouse_workspace_id": "d5a49b19-e41b-46bb-bd89-f751dac1583c",
    "known_lakehouses": [
     {
      "id": "7a50a8b4-1a06-410a-945e-9d0db53c9328"
     }
    ]
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
