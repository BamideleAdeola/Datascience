{"cells":[{"cell_type":"code","source":["table_name = \"sales\"\n","\n","# change the cell to toggle parameter cell"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1830cbda-7268-45c4-b149-100c624bbf61","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-08T07:24:36.6209679Z","session_start_time":null,"execution_start_time":"2023-07-08T07:24:37.299889Z","execution_finish_time":"2023-07-08T07:24:37.6804054Z","spark_jobs":{"numbers":{"SUCCEEDED":0,"FAILED":0,"RUNNING":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"2ded75da-e78a-4d8e-97d5-65fa3b78c6e0"},"text/plain":"StatementMeta(, 1830cbda-7268-45c4-b149-100c624bbf61, 7, Finished, Available)"},"metadata":{}}],"execution_count":5,"metadata":{"tags":[]},"id":"1fa5d79b-6156-4884-b730-7ec019df7832"},{"cell_type":"code","source":["from pyspark.sql.functions import *\r\n","\r\n","# Read the new sales data\r\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\r\n","\r\n","## Add month and year columns\r\n","df = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\r\n","\r\n","# Derive FirstName and LastName columns\r\n","df = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\r\n","\r\n","# Filter and reorder columns\r\n","df = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\r\n","\r\n","# Load the data into a table\r\n","df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"1830cbda-7268-45c4-b149-100c624bbf61","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2023-07-08T07:24:36.8169314Z","session_start_time":null,"execution_start_time":"2023-07-08T07:24:38.0943108Z","execution_finish_time":"2023-07-08T07:24:45.0274635Z","spark_jobs":{"numbers":{"SUCCEEDED":5,"FAILED":0,"RUNNING":0,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4469,"rowCount":50,"jobId":16,"name":"toString at String.java:2994","description":"Delta: Job group for statement 8:\nfrom pyspark.sql.functions import *\n\n# Read the new sales data\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n\n## Add month and year columns\ndf = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n\n# Derive FirstName and LastName columns\ndf = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Filter and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n# Load the data into a table\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(table_name): Compute snapshot for version: 1","submissionTime":"2023-07-08T07:24:44.165GMT","completionTime":"2023-07-08T07:24:44.210GMT","stageIds":[24,25,23],"jobGroup":"8","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":52,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4469,"dataRead":3490,"rowCount":56,"jobId":15,"name":"toString at String.java:2994","description":"Delta: Job group for statement 8:\nfrom pyspark.sql.functions import *\n\n# Read the new sales data\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n\n## Add month and year columns\ndf = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n\n# Derive FirstName and LastName columns\ndf = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Filter and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n# Load the data into a table\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(table_name): Compute snapshot for version: 1","submissionTime":"2023-07-08T07:24:43.457GMT","completionTime":"2023-07-08T07:24:44.141GMT","stageIds":[21,22],"jobGroup":"8","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":3490,"dataRead":4328,"rowCount":12,"jobId":14,"name":"toString at String.java:2994","description":"Delta: Job group for statement 8:\nfrom pyspark.sql.functions import *\n\n# Read the new sales data\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n\n## Add month and year columns\ndf = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n\n# Derive FirstName and LastName columns\ndf = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Filter and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n# Load the data into a table\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(table_name): Compute snapshot for version: 1","submissionTime":"2023-07-08T07:24:43.225GMT","completionTime":"2023-07-08T07:24:43.306GMT","stageIds":[20],"jobGroup":"8","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":2,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":2,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":456385,"dataRead":3311550,"rowCount":65436,"jobId":13,"name":"saveAsTable at NativeMethodAccessorImpl.java:0","description":"Job group for statement 8:\nfrom pyspark.sql.functions import *\n\n# Read the new sales data\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n\n## Add month and year columns\ndf = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n\n# Derive FirstName and LastName columns\ndf = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Filter and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n# Load the data into a table\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)","submissionTime":"2023-07-08T07:24:42.133GMT","completionTime":"2023-07-08T07:24:42.747GMT","stageIds":[19],"jobGroup":"8","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":65536,"rowCount":1,"jobId":12,"name":"load at NativeMethodAccessorImpl.java:0","description":"Job group for statement 8:\nfrom pyspark.sql.functions import *\n\n# Read the new sales data\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n\n## Add month and year columns\ndf = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n\n# Derive FirstName and LastName columns\ndf = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Filter and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n# Load the data into a table\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)","submissionTime":"2023-07-08T07:24:39.448GMT","completionTime":"2023-07-08T07:24:39.563GMT","stageIds":[18],"jobGroup":"8","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"b997f1f3-fa2b-4b27-9acd-63eff6a73ad6"},"text/plain":"StatementMeta(, 1830cbda-7268-45c4-b149-100c624bbf61, 8, Finished, Available)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f8ebd533-60b8-4fae-9538-4427b404714f"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"9c461061-3322-441e-8f30-87bc250d34f7"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"95cfe828-7d18-4d84-b294-c8c5d0ad3079"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4ad2e390-618d-4c03-8d88-4da0b9b804a6"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"61b6d816-9c5d-4489-a4b4-624a4bba5665"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"6fac1cea-efd9-4f3e-be04-c6424321a4f7","known_lakehouses":[{"id":"6fac1cea-efd9-4f3e-be04-c6424321a4f7"}],"default_lakehouse_name":"pipeline_lakehouse","default_lakehouse_workspace_id":"ffb88816-6c81-48bf-a09a-6468ccede35e"}}},"nbformat":4,"nbformat_minor":5}